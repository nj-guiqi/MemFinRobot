"""åˆè§„å®¡æ ¡å™¨ - ç¡®ä¿è¾“å‡ºç¬¦åˆç›‘ç®¡è¦æ±‚"""

import logging
import re
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

from memfinrobot.memory.schemas import UserProfile, RiskLevel
from memfinrobot.config.settings import Settings, get_settings

logger = logging.getLogger(__name__)


@dataclass
class ComplianceResult:
    """åˆè§„æ£€æŸ¥ç»“æœ"""
    is_compliant: bool = True
    needs_modification: bool = False
    original_content: str = ""
    modified_content: str = ""
    violations: List[Dict[str, Any]] = field(default_factory=list)
    risk_disclaimer_added: bool = False
    suitability_warning: Optional[str] = None


class ComplianceGuard:
    """
    åˆè§„å®¡æ ¡å™¨
    
    èŒè´£ï¼š
    1. é€‚å½“æ€§æ£€æŸ¥ï¼šç”¨æˆ·é£é™©ç­‰çº§ vs å»ºè®®å†…å®¹é£é™©ç­‰çº§
    2. ç¦è¯­/é«˜é£é™©è¡¨è¾¾è¿‡æ»¤
    3. å¼ºåˆ¶æ·»åŠ é£é™©æç¤º
    """
    
    # ç¦è¯­åˆ—è¡¨åŠå…¶å¤„ç†æ–¹å¼
    FORBIDDEN_PATTERNS = [
        # (æ­£åˆ™æ¨¡å¼, è¿è§„ç±»å‹, æ›¿æ¢å»ºè®®)
        (r"ä¿è¯.*?æ”¶ç›Š", "promise_return", "æŠ•èµ„æ”¶ç›Šä¸ç¡®å®šï¼Œæ— æ³•ä¿è¯"),
        (r"ç¨³èµš|å¿…æ¶¨|å¿…èµš", "guarantee", "æŠ•èµ„å­˜åœ¨é£é™©ï¼Œä¸èƒ½ä¿è¯ç›ˆåˆ©"),
        (r"å†…å¹•|å°é“æ¶ˆæ¯", "insider", "è¯·ä»¥å…¬å¼€ä¿¡æ¯ä¸ºä¾æ®"),
        (r"èè‚¡|æ¨è.*?(ä¹°å…¥|å–å‡º)", "recommendation", "ä»¥ä¸Šä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®"),
        (r"(ä¹°å…¥|å–å‡º|å»ºä»“|åŠ ä»“|å‡ä»“|æ¸…ä»“).*?(ç‚¹ä½|ä»·æ ¼)", "trading_advice", "å…·ä½“äº¤æ˜“å†³ç­–è¯·æ‚¨è‡ªè¡Œåˆ¤æ–­"),
        (r"ä¸€å®š(ä¼š|èƒ½|æ¶¨|è·Œ)", "certainty", "å¸‚åœºå­˜åœ¨ä¸ç¡®å®šæ€§"),
        (r"ç»å¯¹(å®‰å…¨|æ²¡é—®é¢˜)", "absolute", "ä»»ä½•æŠ•èµ„éƒ½å­˜åœ¨é£é™©"),
    ]
    
    # é£é™©æç¤ºæ¨¡æ¿
    DEFAULT_RISK_DISCLAIMER = (
        "\n\nã€é£é™©æç¤ºã€‘ä»¥ä¸Šå†…å®¹ä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚"
        "æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚è¯·æ ¹æ®è‡ªèº«é£é™©æ‰¿å—èƒ½åŠ›è°¨æ…å†³ç­–ã€‚"
    )
    
    # é€‚å½“æ€§æç¤ºæ¨¡æ¿
    SUITABILITY_TEMPLATES = {
        "high_risk_to_low_user": (
            "\n\nâš ï¸ æ¸©é¦¨æç¤ºï¼šæ‚¨å½“å‰çš„é£é™©æ‰¿å—èƒ½åŠ›è¯„ä¼°ä¸ºè¾ƒä½æ°´å¹³ï¼Œ"
            "è€Œä¸Šè¿°æåŠçš„äº§å“/ç­–ç•¥é£é™©ç­‰çº§è¾ƒé«˜ã€‚å»ºè®®æ‚¨å……åˆ†äº†è§£ç›¸å…³é£é™©åå†åšå†³å®šï¼Œ"
            "æˆ–è€ƒè™‘é£é™©ç­‰çº§æ›´åŒ¹é…çš„æŠ•èµ„æ–¹å¼ã€‚"
        ),
        "incomplete_profile": (
            "\n\nğŸ’¡ ä¸ºäº†æä¾›æ›´é€‚åˆæ‚¨çš„å»ºè®®ï¼Œæ‚¨æ˜¯å¦æ–¹ä¾¿å‘Šè¯‰æˆ‘ï¼š\n"
            "1. æ‚¨çš„æŠ•èµ„ç»éªŒå¦‚ä½•ï¼Ÿ\n"
            "2. æ‚¨èƒ½æ¥å—çš„æœ€å¤§äºæŸæ˜¯å¤šå°‘ï¼Ÿ\n"
            "3. æ‚¨çš„æŠ•èµ„æœŸé™å¤§æ¦‚æ˜¯å¤šä¹…ï¼Ÿ"
        ),
    }
    
    def __init__(
        self,
        settings: Optional[Settings] = None,
        forbidden_phrases: Optional[List[str]] = None,
    ):
        """
        åˆå§‹åŒ–åˆè§„å®¡æ ¡å™¨
        
        Args:
            settings: é…ç½®å¯¹è±¡
            forbidden_phrases: é¢å¤–çš„ç¦è¯­åˆ—è¡¨
        """
        self.settings = settings or get_settings()
        
        # åˆå¹¶ç¦è¯­åˆ—è¡¨
        self.forbidden_phrases = list(self.settings.compliance.forbidden_phrases)
        if forbidden_phrases:
            self.forbidden_phrases.extend(forbidden_phrases)
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.compiled_patterns = [
            (re.compile(pattern, re.IGNORECASE), vtype, replacement)
            for pattern, vtype, replacement in self.FORBIDDEN_PATTERNS
        ]
    
    def check(
        self,
        content: str,
        user_profile: Optional[UserProfile] = None,
        content_risk_level: Optional[str] = None,
        force_disclaimer: bool = True,
    ) -> ComplianceResult:
        """
        æ‰§è¡Œåˆè§„æ£€æŸ¥
        
        Args:
            content: è¦æ£€æŸ¥çš„å†…å®¹
            user_profile: ç”¨æˆ·ç”»åƒ
            content_risk_level: å†…å®¹æ¶‰åŠçš„é£é™©ç­‰çº§
            force_disclaimer: æ˜¯å¦å¼ºåˆ¶æ·»åŠ é£é™©æç¤º
            
        Returns:
            ComplianceResult åŒ…å«æ£€æŸ¥ç»“æœå’Œä¿®æ”¹å»ºè®®
        """
        result = ComplianceResult(
            original_content=content,
            modified_content=content,
        )
        
        # 1. æ£€æŸ¥ç¦è¯­
        self._check_forbidden_phrases(result)
        
        # 2. æ£€æŸ¥ç¦è¯­æ¨¡å¼
        self._check_forbidden_patterns(result)
        
        # 3. é€‚å½“æ€§æ£€æŸ¥
        if user_profile:
            self._check_suitability(result, user_profile, content_risk_level)
        
        # 4. æ£€æŸ¥/æ·»åŠ é£é™©æç¤º
        if force_disclaimer:
            self._ensure_risk_disclaimer(result)
        
        # æ›´æ–°åˆè§„çŠ¶æ€
        result.is_compliant = len(result.violations) == 0
        result.needs_modification = (
            result.modified_content != result.original_content
        )
        
        return result
    
    def _check_forbidden_phrases(self, result: ComplianceResult) -> None:
        """æ£€æŸ¥ç®€å•ç¦è¯­"""
        content = result.modified_content
        
        for phrase in self.forbidden_phrases:
            if phrase in content:
                result.violations.append({
                    "type": "forbidden_phrase",
                    "phrase": phrase,
                    "severity": "high",
                })
                # æ›¿æ¢ç¦è¯­
                content = content.replace(phrase, f"[{phrase}ï¼ˆå·²åˆ é™¤ï¼‰]")
        
        result.modified_content = content
    
    def _check_forbidden_patterns(self, result: ComplianceResult) -> None:
        """æ£€æŸ¥ç¦è¯­æ¨¡å¼"""
        content = result.modified_content
        
        for pattern, vtype, replacement in self.compiled_patterns:
            matches = pattern.findall(content)
            if matches:
                for match in matches:
                    result.violations.append({
                        "type": vtype,
                        "match": match if isinstance(match, str) else match[0],
                        "severity": "medium",
                    })
                # æ·»åŠ ä¿®æ­£è¯´æ˜
                content = pattern.sub(f"ï¼ˆ{replacement}ï¼‰", content)
        
        result.modified_content = content
    
    def _check_suitability(
        self,
        result: ComplianceResult,
        user_profile: UserProfile,
        content_risk_level: Optional[str] = None,
    ) -> None:
        """é€‚å½“æ€§æ£€æŸ¥"""
        # æ£€æµ‹å†…å®¹é£é™©ç­‰çº§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        if content_risk_level is None:
            content_risk_level = self._detect_content_risk_level(result.modified_content)
        
        # ç”¨æˆ·é£é™©ç­‰çº§
        user_risk = user_profile.risk_level
        
        # é€‚å½“æ€§åŒ¹é…æ£€æŸ¥
        if content_risk_level == "high" and user_risk == RiskLevel.LOW:
            result.suitability_warning = self.SUITABILITY_TEMPLATES["high_risk_to_low_user"]
            result.violations.append({
                "type": "suitability_mismatch",
                "user_risk": user_risk.value,
                "content_risk": content_risk_level,
                "severity": "warning",
            })
        
        # ç”»åƒä¸å®Œæ•´æç¤º
        if user_risk == RiskLevel.UNKNOWN:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¢é—®ç”»åƒçš„å†…å®¹
            if "é£é™©æ‰¿å—" not in result.modified_content and "æŠ•èµ„ç»éªŒ" not in result.modified_content:
                result.suitability_warning = self.SUITABILITY_TEMPLATES["incomplete_profile"]
    
    def _detect_content_risk_level(self, content: str) -> str:
        """æ£€æµ‹å†…å®¹æ¶‰åŠçš„é£é™©ç­‰çº§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        high_risk_keywords = ["è‚¡ç¥¨", "æœŸè´§", "æœŸæƒ", "æ æ†", "é«˜æ³¢åŠ¨", "é«˜é£é™©"]
        medium_risk_keywords = ["æ··åˆå‹", "åè‚¡", "æŒ‡æ•°åŸºé‡‘", "ETF"]
        low_risk_keywords = ["è´§å¸åŸºé‡‘", "å€ºåˆ¸", "é“¶è¡Œç†è´¢", "å­˜æ¬¾", "ä½é£é™©"]
        
        content_lower = content.lower()
        
        high_count = sum(1 for kw in high_risk_keywords if kw in content_lower)
        medium_count = sum(1 for kw in medium_risk_keywords if kw in content_lower)
        low_count = sum(1 for kw in low_risk_keywords if kw in content_lower)
        
        if high_count > 0 and high_count >= medium_count:
            return "high"
        elif medium_count > 0:
            return "medium"
        elif low_count > 0:
            return "low"
        else:
            return "unknown"
    
    def _ensure_risk_disclaimer(self, result: ComplianceResult) -> None:
        """ç¡®ä¿åŒ…å«é£é™©æç¤º"""
        content = result.modified_content
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰é£é™©æç¤º
        disclaimer_keywords = ["é£é™©æç¤º", "æŠ•èµ„æœ‰é£é™©", "å…¥å¸‚éœ€è°¨æ…"]
        has_disclaimer = any(kw in content for kw in disclaimer_keywords)
        
        if not has_disclaimer:
            # æ·»åŠ é£é™©æç¤º
            content = content.rstrip() + self.settings.compliance.risk_disclaimer
            result.risk_disclaimer_added = True
        
        # æ·»åŠ é€‚å½“æ€§è­¦å‘Šï¼ˆå¦‚æœæœ‰ï¼‰
        if result.suitability_warning:
            content = content.rstrip() + result.suitability_warning
        
        result.modified_content = content
    
    def filter_response(
        self,
        content: str,
        user_profile: Optional[UserProfile] = None,
    ) -> str:
        """
        è¿‡æ»¤å“åº”å†…å®¹çš„ä¾¿æ·æ–¹æ³•
        
        Args:
            content: åŸå§‹å†…å®¹
            user_profile: ç”¨æˆ·ç”»åƒ
            
        Returns:
            è¿‡æ»¤åçš„å†…å®¹
        """
        result = self.check(content, user_profile)
        return result.modified_content
